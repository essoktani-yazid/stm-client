import json
import re
from fastapi import WebSocketDisconnect
from app.database import db
from app.services.ai_service import stream_llm_response
from app.rag import prompts
from app.utils.logger import log

# --- UTILITAIRES ---
def _fix_control_chars_in_strings(s: str) -> str:
    """√âchappe les caract√®res de contr√¥le (ex: retours √† la ligne) dans les valeurs des cha√Ænes JSON."""
    def replacer(match):
        content = match.group(1)
        content = content.replace("\r", "\\r").replace("\n", "\\n").replace("\t", "\\t")
        for i in range(32):
            if i not in (9, 10, 13):
                content = content.replace(chr(i), " ")
        return '"' + content + '"'
    return re.sub(r'"((?:[^"\\]|\\.)*)"', replacer, s)


def clean_llm_json(response: str) -> str:
    """
    Nettoie la r√©ponse LLM pour extraction JSON.
    Retire les balises markdown ```json et ``` si pr√©sentes.
    Corrige les caract√®res de contr√¥le invalides (retours √† la ligne bruts dans les cha√Ænes).
    
    Args:
        response: La r√©ponse brute du LLM
        
    Returns:
        JSON nettoy√© sans balises markdown
    """
    cleaned = response.strip()
    
    # Retirer balises markdown ```json
    if cleaned.startswith("```json"):
        cleaned = cleaned[7:]  # Enlever ```json
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]  # Enlever ```
        cleaned = cleaned.strip()
    # Retirer balises markdown ```
    elif cleaned.startswith("```"):
        cleaned = cleaned[3:]  # Enlever ```
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]  # Enlever ```
        cleaned = cleaned.strip()
    
    # Corriger les caract√®res de contr√¥le invalides dans les cha√Ænes JSON
    cleaned = _fix_control_chars_in_strings(cleaned)
    
    # Extraire uniquement l'objet JSON (ignorer texte avant/apr√®s)
    # Le LLM ajoute parfois des notes apr√®s le JSON: *(Note: ...)*
    start = cleaned.find('{')
    if start != -1:
        depth = 0
        in_string = False
        escape = False
        for i, ch in enumerate(cleaned[start:], start):
            if escape:
                escape = False
                continue
            if ch == '\\' and in_string:
                escape = True
                continue
            if ch == '"' and not escape:
                in_string = not in_string
                continue
            if not in_string:
                if ch == '{':
                    depth += 1
                elif ch == '}':
                    depth -= 1
                    if depth == 0:
                        cleaned = cleaned[start:i+1]
                        break
    
    return cleaned

# --- UTILITAIRES D'ENVOI S√âCURIS√â ---
async def safe_send(websocket, data: dict):
    """
    Envoie des donn√©es au client WebSocket de mani√®re s√©curis√©e.
    Retourne False si le client est d√©connect√©, True sinon.
    """
    try:
        await websocket.send_json(data)
        return True
    except (WebSocketDisconnect, RuntimeError):
        log.warning("Client d√©connect√© pendant l'envoi.")
        return False

async def send_status(websocket, message):
    """Envoie un message de statut interm√©diaire (pour le spinner)."""
    return await safe_send(websocket, {"status": message})

# --- LOGIQUE PRINCIPALE ---
async def handle_user_request(websocket, user_id, user_message):
    log.info("--- D√©but du traitement ---")
    
    # 1. ANALYSE DE L'INTENTION
    # On envoie un statut pour activer le spinner c√¥t√© client
    if not await send_status(websocket, "Analyzing user intent..."): return
    
    # Appel au LLM pour obtenir le JSON (Operation + SQL)
    system_prompt = prompts.get_system_prompt_sql(user_id)
    # IMPORTANT: return_as_string=True pour parser le JSON
    ai_res = await stream_llm_response(user_message, system_prompt, return_as_string=True)
    
    try:
        # Nettoyer la r√©ponse si elle contient des balises markdown
        cleaned_res = clean_llm_json(ai_res)
        data = json.loads(cleaned_res)
        op = data["operation_type"]
        # Normaliser INFO ‚Üí INFORMATION
        if op == "INFO":
            op = "INFORMATION"
        query = data.get("sql_query", None)  # Peut √™tre absent pour INFORMATION
        log.ai(f"Op√©ration : {op} | SQL : {query}")
    except (KeyError, json.JSONDecodeError) as e:
        log.error(f"Erreur parsing JSON IA : {e}")
        log.error(f"R√©ponse re√ßue : {ai_res}")
        await safe_send(websocket, {"display_message": "‚ùå Error analyzing request. Please try again.", "requires_confirmation": False})
        return

    # 2. TRAITEMENT SELON LE TYPE D'OP√âRATION
    
    # =========================================================================
    # CAS 1 : LECTURE (READ)
    # =========================================================================
    if op == "READ":
        if not await send_status(websocket, "Reading database..."): return
        
        try:
            # Ex√©cution directe
            rows = db.execute(query).fetchall()
            log.db(f"Lignes r√©cup√©r√©es : {len(rows)}")
        except Exception as e:
            await safe_send(websocket, {"display_message": f"‚ùå Erreur SQL: {e}", "requires_confirmation": False})
            return

        if not await send_status(websocket, f"Found {len(rows)} items. Summarizing..."): return
        
        # G√©n√©ration du r√©sum√© naturel via le LLM (en string compl√®te)
        final_text = await stream_llm_response(
            prompts.get_final_answer_prompt(user_message, rows), 
            "You are a helpful assistant.",
            return_as_string=True
        )
        
        await safe_send(websocket, {"display_message": final_text, "requires_confirmation": False})

    # =========================================================================
    # CAS 2 : CR√âATION (CREATE)
    # =========================================================================
    elif op == "CREATE":
        if not await send_status(websocket, "Preparing creation request..."): return
        
        # On demande √† l'IA de formuler une confirmation propre
        confirm_text = await stream_llm_response(
            prompts.get_confirmation_prompt(query), 
            "Assistant de confirmation.",
            return_as_string=True
        )
        
        await safe_send(websocket, {
            "display_message": confirm_text,
            "requires_confirmation": True, # Boutons visibles
            "sql_to_execute": query,
            "operation_type": "CREATE"
        })

    # =========================================================================
    # CAS 3 : MODIFICATION / SUPPRESSION (UPDATE / DELETE)
    # =========================================================================
    elif op in ["UPDATE", "DELETE"]:
        if not await send_status(websocket, f"Calculating impact for {op}..."): return
        
        # BLOC DE S√âCURIT√â GLOBAL (Emp√™che le spinner infini)
        try:
            # A. Construction de la requ√™te de pr√©visualisation (SELECT)
            preview_query = query.replace("DELETE FROM", "SELECT * FROM").replace("UPDATE", "SELECT * FROM")
            
            # Gestion des UPDATE complexes (ex: UPDATE tasks SET ... WHERE ...)
            if "SET" in preview_query:
                try:
                    table_part = "tasks" if "tasks" in query else "sub_tasks"
                    if "WHERE" in query.upper():
                        idx = query.upper().find("WHERE")
                        where_part = query[idx:]
                        preview_query = f"SELECT * FROM {table_part} {where_part}"
                    else:
                        # S√©curit√© si pas de WHERE : on limite pour voir un √©chantillon
                        preview_query = f"SELECT * FROM {table_part} LIMIT 5"
                except Exception as parsing_error:
                    log.error(f"Erreur parsing preview: {parsing_error}")
                    preview_query = query # Fallback (risqu√© mais rare)

            # B. Ex√©cution de la simulation
            rows = []
            count = 0
            try:
                cursor = db.execute(preview_query)
                if cursor:
                    rows = cursor.fetchall()
                    count = len(rows)
            except Exception as sql_error:
                log.error(f"Erreur SQL Simulation: {sql_error}")
                rows = []
                count = 0

            # C. Logique de r√©ponse selon le nombre de r√©sultats

            # --- SOUS-CAS : AUCUN R√âSULTAT (0) ---
            if count == 0:
                log.ai("0 r√©sultat trouv√©. Demande d'explication √† l'IA...")
                
                no_result_msg = await stream_llm_response(
                    prompts.get_no_results_prompt(user_message, query),
                    "You are a helpful assistant.",
                    return_as_string=True
                )

                # IMPORTANT : requires_confirmation=False pour ne pas afficher les boutons
                await safe_send(websocket, {
                    "display_message": no_result_msg,
                    "requires_confirmation": False, 
                    "sql_to_execute": None
                })
            
            # --- SOUS-CAS : R√âSULTATS TROUV√âS (> 0) ---
            else:
                # 1. Pr√©paration des exemples
                examples_list = []
                for row in rows[:3]: 
                    # Gestion robuste du titre
                    title = row.get('title') or row.get('description') or f"Task #{row.get('id')}"
                    examples_list.append(f"‚Ä¢ {title}")
                
                examples_str = "\n".join(examples_list)
                if len(rows) > 3:
                    examples_str += f"\n‚Ä¢ ... and {len(rows)-3} others."

                # 2. Tentative de g√©n√©ration par l'IA avec Fallback (S√©curit√©)
                log.ai(f"G√©n√©ration avertissement pour {count} items...")
                
                warning_msg = ""
                try:
                    # On essaie d'appeler l'IA
                    warning_msg = await stream_llm_response(
                        prompts.get_impact_warning_prompt(user_message, count, examples_str, op), # On passe 'op'
                        "Security Assistant",
                        return_as_string=True
                    )
                except Exception as llm_error:
                    # SI L'IA PLANTE (KeyError 'choices', Timeout, etc.)
                    log.error(f"‚ö†Ô∏è L'IA n'a pas pu g√©n√©rer l'avertissement : {llm_error}")
                    # => On utilise un message standard de secours
                    warning_msg = f"‚ö†Ô∏è **Attention** : This action will affect **{count}** task(s).\n\n"
                    warning_msg += f"Examples:\n{examples_str}\n\n"
                    warning_msg += "**System Fallback:** Could not verify with AI, please confirm carefully.\n"
                    warning_msg += "Do you want to proceed?"

                # 3. Envoi du message (IA ou Secours)
                await safe_send(websocket, {
                    "display_message": warning_msg,
                    "requires_confirmation": True, 
                    "sql_to_execute": query,
                    "operation_type": op
                })

        except Exception as critical_error:
            # Filet de s√©curit√© ultime : Si tout plante, on pr√©vient le client
            log.error(f"CRASH CRITIQUE DANS UPDATE/DELETE: {critical_error}")
            await safe_send(websocket, {
                "display_message": f"‚ùå An internal error occurred.\nDetails: {str(critical_error)}",
                "requires_confirmation": False
            })
    # =========================================================================
    # CAS 4 : INFORMATION (Requ√™te conversationnelle, pas de SQL)
    # =========================================================================
    elif op == "INFORMATION":
        # La r√©ponse conversationnelle est d√©j√† dans le JSON du LLM
        response_text = data.get("response", "I'm here to help you manage your tasks! Try asking me something like 'show my tasks'.")
        
        await safe_send(websocket, {
            "display_message": response_text,
            "requires_confirmation": False
        })
    
    log.info("--- Fin du traitement ---")


async def execute_and_summarize(websocket, sql_query):
    """
    Ex√©cute le SQL confirm√©, v√©rifie le r√©sultat, et utilise l'IA pour la r√©ponse finale.
    """
    log.info(f"--- Ex√©cution confirm√©e : {sql_query} ---")
    
    # 1. Feedback visuel : Ex√©cution en cours
    if not await send_status(websocket, "Executing operation..."): return

    execution_result = ""
    is_success = True

    # 2. Ex√©cution r√©elle en base de donn√©es
    try:
        # On utilise db.execute qui retourne un curseur (via SQLAlchemy ou connecteur brut)
        cursor = db.execute(sql_query)
        # On tente de r√©cup√©rer le nombre de lignes affect√©es (rowcount)
        row_count = cursor.rowcount if hasattr(cursor, 'rowcount') else "unknown"
        
        execution_result = f"Success. Rows affected: {row_count}"
        log.db(f"Ex√©cution OK. Rows: {row_count}")
        
    except Exception as e:
        is_success = False
        execution_result = f"Error: {str(e)}"
        log.error(f"Erreur SQL execution: {e}")

    # 3. Feedback visuel : Analyse du r√©sultat
    if not await send_status(websocket, "Analyzing result..."): return

    # 4. Appel au LLM pour g√©n√©rer la r√©ponse post-action
    # On cr√©e un prompt 'ad-hoc' ou on l'ajoute dans prompts.py
    summary_prompt = (
        f"The user ordered an operation. Here is the SQL executed: \"{sql_query}\".\n"
        f"Here is the database execution result: \"{execution_result}\".\n"
        "Generate a short, natural language response confirming the action to the user."
        "If it failed, apologize and explain why."
    )

    final_message = await stream_llm_response(
        summary_prompt, 
        "You are a helpful task manager assistant.",
        return_as_string=True
    )

    # 5. Envoi de la r√©ponse finale au client
    # Le client affichera le message et arr√™tera le spinner
    await safe_send(websocket, {
        "display_message": final_message,
        "requires_confirmation": False
    })

async def generate_dashboard_insights(websocket, user_stats):
    """
    G√©n√®re l'analyse pour le Dashboard (Smart Header).
    """
    try:
        # 1. Pr√©parer le prompt
        stats_str = json.dumps(user_stats)
        prompt_text = prompts.get_dashboard_insight_prompt(stats_str)
        
        log.ai(f"üìä G√©n√©ration insights dashboard...")
        log.ai(f"Stats: {stats_str}")
        
        # 2. Appel au LLM pour obtenir le JSON d'insights
        content = await stream_llm_response(
            prompt=prompt_text,
            system_content="Productivity Coach",
            return_as_string=True  # On veut un JSON complet
        )
        
        log.ai(f"R√©ponse LLM brute: {content[:100]}...")
        
        # 2.5 Nettoyer les balises markdown si pr√©sentes
        cleaned_content = clean_llm_json(content)
        
        log.ai(f"JSON nettoy√©: {cleaned_content[:100]}...")
        
        # 2.6 Valider que c'est du JSON valide avant d'envoyer
        try:
            json.loads(cleaned_content)  # Test de validation
            log.success(f"‚úÖ JSON d'insights valide g√©n√©r√©")
        except json.JSONDecodeError as json_err:
            log.error(f"‚ùå JSON invalide apr√®s nettoyage: {json_err}")
            log.error(f"Contenu: {cleaned_content}")
            raise
        
        # 3. Envoi au client Java
        await websocket.send_text(cleaned_content)
        log.success(f"üì§ Insights envoy√©s au dashboard")
        
    except Exception as e:
        log.error(f"Erreur lors de la g√©n√©ration d'insights : {e}")
        import traceback
        traceback.print_exc()
        
        # Fallback avec valeurs par d√©faut
        error_json = {
            "mood": "ü§ñ",
            "title": "Dashboard Ready",
            "message": "Your productivity data is being analyzed. Check back soon!",
            "theme_color": "#6366F1",
            "action_label": None
        }
        await websocket.send_text(json.dumps(error_json))